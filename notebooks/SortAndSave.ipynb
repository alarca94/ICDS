{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.chdir('..')\n",
    "import nltk\n",
    "from utils import functions\n",
    "import spacy\n",
    "\n",
    "from nltk.parse.corenlp import CoreNLPParser, CoreNLPDependencyParser\n",
    "from gensim.models import KeyedVectors\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './DATA_SETS'\n",
    "\n",
    "mens_path = 'MEN'\n",
    "sick_path = 'SICK'\n",
    "sts_path = 'STS'\n",
    "\n",
    "parsers = {'seq': CoreNLPParser(url='http://localhost:9000'),\n",
    "           'syn': CoreNLPParser(url='http://localhost:9000'),\n",
    "           'dep': CoreNLPDependencyParser(url='http://localhost:9000')}\n",
    "\n",
    "column_names = ['identifier', 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_text(text, criteria, parsers, start):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    \n",
    "    if len(sents) > 1:\n",
    "        return [functions.sort_def(s, criteria=criteria, parser=parsers[criteria], start=start) for s in sents]\n",
    "    else:\n",
    "        return functions.sort_def(sents[0], criteria=criteria, parser=parsers[criteria], start=start)\n",
    "\n",
    "def check_roots(tree, text):\n",
    "    n_roots = 0\n",
    "    for key, node in tree.nodes.items():\n",
    "        if node['head'] is None:\n",
    "            n_roots += 1\n",
    "    if n_roots != 1:\n",
    "        print('ERROR:N_ROOTS // ' + text)\n",
    "            \n",
    "def check_parents(tree, text):\n",
    "    addresses = []\n",
    "    for key, node in tree.nodes.items():\n",
    "        if type(node['head']) is list and len(node['head'] > 1):\n",
    "            print('ERROR:MULTIPLE_HEADS // ' + text + ' // ' + node['word'])\n",
    "        if node['address'] in addresses:\n",
    "            print('ERROR:MULTIPLE_HEADS // ' + text + ' // ' + node['word'])\n",
    "        addresses.append(node['address'])\n",
    "            \n",
    "def check_free_nodes(tree, tokens):\n",
    "    t_tokens = []\n",
    "    skip_tokens = list('()\"`.[]—{}') + [\"''\", '’’', '‘‘']\n",
    "    for key, node in tree.nodes.items():\n",
    "        t_tokens.append(node['word'])\n",
    "        \n",
    "    missing_tokens = tokens[~tokens.isin(t_tokens) & ~tokens.isin(skip_tokens)].tolist()\n",
    "    \n",
    "    if missing_tokens:\n",
    "        print('ERROR:MULTIPLE_FREE_NODES // ' + str(tokens.tolist()) + ' // ' + str(missing_tokens))\n",
    "\n",
    "def check_tree(text, criteria, parsers, start):\n",
    "    sents = nltk.sent_tokenize(text)\n",
    "    \n",
    "    if len(sents) > 1:\n",
    "        for s in sents:\n",
    "            trees = list(parsers[criteria].raw_parse(s, keepPunct=True))\n",
    "            \n",
    "            # if len(trees) != 1:\n",
    "            #     print('ERROR:N_TREES // ' + s)\n",
    "                \n",
    "            tree = trees[0]\n",
    "            # check_roots(tree, s)\n",
    "            check_parents(tree, s)\n",
    "            # check_free_nodes(tree, pd.Series(parsers[criteria].tokenize(s)))\n",
    "            \n",
    "            # if tree.contains_cycle():\n",
    "            #     print('ERROR:CONTAINS_CYCLE // ' + s)\n",
    "    else:\n",
    "        trees = list(parsers[criteria].raw_parse(sents[0], keepPunct=True))\n",
    "            \n",
    "        # if len(trees) != 1:\n",
    "        #     print('ERROR:N_TREES // ' + text)\n",
    "\n",
    "        tree = trees[0]\n",
    "        # check_roots(tree, text)\n",
    "        check_parents(tree, text)\n",
    "        # check_free_nodes(tree, pd.Series(parsers[criteria].tokenize(text)))\n",
    "        \n",
    "        # if tree.contains_cycle():\n",
    "        #     print('ERROR:CONTAINS_CYCLE // ' + text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file MEN_SENTENCES.txt...\n",
      "Processing file SICK.SENTENCES.txt...\n",
      "Processing file STS.TEST.SENTENCES.txt...\n"
     ]
    }
   ],
   "source": [
    "for path in [mens_path, sick_path, sts_path]:\n",
    "    fname = [f for f in os.listdir(os.path.join(data_path, path)) if f.endswith('SENTENCES.txt')][0]\n",
    "    print('Processing file ' + fname + '...')\n",
    "\n",
    "    with open(os.path.join(data_path, path, fname), 'r', encoding='utf-8') as f:\n",
    "        data = pd.read_csv(f, index_col=0, sep='\\t', header=None, names=column_names)\n",
    "\n",
    "    criteria = 'dep'\n",
    "    data[['text']].apply(lambda x: check_tree(x['text'], criteria, parsers, 'right'), \n",
    "                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file STS.TEST.SENTENCES.txt...\n",
      "Number of sentences to sort: 2204\n",
      "Sorting texts according to seq...\n",
      "Sorting texts according to syn...\n",
      "Sorting texts according to dep, starting at the left...\n",
      "Adding missed tokens: ['Corp'] /// Electronic Data Systems Corp. Thursday said the Securities and Exchange Commission has asked the company for documents related to its large contract with the U.S. Navy.\n",
      "Adding missed tokens: ['Va'] /// His sport utility vehicle was found June 25, abandoned without its license plates in Virginia Beach, Va.\n",
      "Adding missed tokens: ['Corp'] /// Nvidia will take advantage of MediaQ customers, which include such players as Siemens AG, Sharp, Philips, Dell, Mitsubishi and Sony Corp.\n",
      "Sorting texts according to dep, starting at the right...\n",
      "Adding missed tokens: ['Corp'] /// Electronic Data Systems Corp. Thursday said the Securities and Exchange Commission has asked the company for documents related to its large contract with the U.S. Navy.\n",
      "Adding missed tokens: ['Va'] /// His sport utility vehicle was found June 25, abandoned without its license plates in Virginia Beach, Va.\n",
      "Adding missed tokens: ['Corp'] /// Nvidia will take advantage of MediaQ customers, which include such players as Siemens AG, Sharp, Philips, Dell, Mitsubishi and Sony Corp.\n",
      "Saving results to csv and pickle files...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for path in [mens_path, sick_path, sts_path]:\n",
    "    fname = [f for f in os.listdir(os.path.join(data_path, path)) if f.endswith('SENTENCES.txt')][0]\n",
    "    print('Processing file ' + fname + '...')\n",
    "    \n",
    "    with open(os.path.join(data_path, path, fname), 'r', encoding='utf-8') as f:\n",
    "        data = pd.read_csv(f, index_col=0, sep='\\t', header=None, names=column_names)\n",
    "    \n",
    "    print('Number of sentences to sort: ' + str(data.shape[0]))\n",
    "    for criteria in parsers:\n",
    "        if criteria == 'dep':\n",
    "            for start in ['left', 'right']:\n",
    "                print('Sorting texts according to ' + criteria + ', starting at the ' + start + '...')\n",
    "                data[criteria + '_' + start] = data[['text']].apply(lambda x: \n",
    "                                                                    sort_text(x['text'], criteria, parsers, start), \n",
    "                                                                    axis=1)\n",
    "        else:\n",
    "            print('Sorting texts according to ' + criteria + '...')\n",
    "            start = 'right'\n",
    "            data[criteria] = data[['text']].apply(lambda x: sort_text(x['text'], criteria, parsers, start), \n",
    "                                                  axis=1)\n",
    "\n",
    "    print('Saving results to csv and pickle files...')\n",
    "    data.to_csv(os.path.join(data_path, path, fname[:-4] + '2.csv'), sep='\\t')\n",
    "    data.to_pickle(os.path.join(data_path, path, fname[:-4] + '2.pkl'))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the chunking to the saved file using spacy's chunker\n",
    "def chunker(text, lang_model, bidirectionality=False):\n",
    "    # tokens = nltk.word_tokenize(text)\n",
    "    p_text = lang_model(text.replace('Near-Earth', 'NearEarth'))\n",
    "    tokens = [token.text for token in p_text]\n",
    "    chunks = [[t.text for t in lang_model(chunk.text.replace('NearEarth', 'Near Earth'))] \n",
    "              for chunk in p_text.noun_chunks]\n",
    "    \n",
    "    if chunks:\n",
    "        t_id = 0\n",
    "        c_text = []\n",
    "\n",
    "        # Keep adding tokens in the original order\n",
    "        for chunk in chunks:\n",
    "            while tokens[t_id] != chunk[0]:\n",
    "                c_text.append(tokens[t_id])\n",
    "                t_id += 1\n",
    "            if len(chunk) == 1:\n",
    "                c_text.extend(chunk)\n",
    "            else:\n",
    "                c_text.append(chunk)\n",
    "            t_id += len(chunk)\n",
    "\n",
    "        # Add posterior tokens to last chunk\n",
    "        last_ix = tokens.index(chunk[-1])\n",
    "        c_text.extend(tokens[last_ix+1:])        \n",
    "    \n",
    "        if len(c_text) == 1 and type(c_text[0]) is list:\n",
    "            c_text = c_text[0]\n",
    "    \n",
    "    else:\n",
    "        c_text = tokens[:]\n",
    "    \n",
    "    # In-Chunks goes right-left and chunks goes left-right\n",
    "    if bidirectionality:\n",
    "        return c_text[::-1]\n",
    "    else:\n",
    "        return c_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file STS.TEST.SENTENCES2.pkl...\n",
      "Saving results to csv and pickle files...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lang_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "for path in [sts_path]:  # mens_path, sick_path, \n",
    "    fname = [f for f in os.listdir(os.path.join(data_path, path)) if f.endswith('SENTENCES2.pkl')][0]\n",
    "    print('Processing file ' + fname + '...')\n",
    "    \n",
    "    data = pd.read_pickle(os.path.join(data_path, path, fname))\n",
    "    data['chunk'] = data[['text']].apply(lambda x: chunker(x['text'], lang_model, False), axis=1)\n",
    "    \n",
    "    print('Saving results to csv and pickle files...')\n",
    "    data.to_csv(os.path.join(data_path, path, fname[:-4] + '.csv'), sep='\\t')\n",
    "    data.to_pickle(os.path.join(data_path, path, fname))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: pencil\n",
      "WORD: scratch\n",
      "WORD: spider\n",
      "WORD: firework\n",
      "Adding missed tokens: ['etc'] /// (usually plural) a device with an explosive that burns at a low rate and with colored flames; can be used to illuminate areas or send signals etc.\n",
      "Adding missed tokens: ['etc'] /// (usually plural) a device with an explosive that burns at a low rate and with colored flames; can be used to illuminate areas or send signals etc.\n",
      "WORD: display\n",
      "WORD: birthday\n",
      "WORD: head\n",
      "WORD: nail\n",
      "WORD: gun\n",
      "WORD: journal\n",
      "WORD: break\n",
      "WORD: tongue\n",
      "WORD: graffito\n",
      "WORD: copper\n",
      "WORD: organ\n",
      "WORD: vintage\n",
      "WORD: pole\n",
      "WORD: donut\n",
      "WORD: ski\n",
      "WORD: keyboard\n",
      "WORD: panda\n",
      "WORD: ipod\n",
      "WORD: ink\n",
      "WORD: beauty\n",
      "WORD: snowman\n",
      "WORD: figure\n",
      "WORD: iris\n",
      "WORD: badge\n",
      "Adding missed tokens: ['etc'] /// an emblem (a small piece of plastic or cloth or metal) that signifies your status (rank or membership or affiliation etc.)\n",
      "Adding missed tokens: ['etc'] /// an emblem (a small piece of plastic or cloth or metal) that signifies your status (rank or membership or affiliation etc.)\n",
      "WORD: doodle\n",
      "Saving results to csv and pickle files...\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# Check which of the MEN words are not available in the MEN_SENTENCES2 dataset and \n",
    "# fill them in with the wordnet definitions\n",
    "sentences = {}\n",
    "experiments = {}\n",
    "experiments_cols = ['text1', 'text2', 'gs']\n",
    "dataset = 'MEN'\n",
    "lang_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "files = os.listdir(os.path.join(data_path, dataset))\n",
    "sentences_file = [f for f in files if f.endswith('SENTENCES2.pkl')][0]\n",
    "experiments_file = [f for f in files if f.endswith('SIMILARITIES.txt')][0]\n",
    "\n",
    "sentences[dataset] = pd.read_pickle(os.path.join(data_path, dataset, sentences_file))\n",
    "experiments[dataset] = pd.read_csv(os.path.join(data_path, dataset, experiments_file), names=experiments_cols,\n",
    "                                  header=None, index_col=0, sep='\\t')\n",
    "\n",
    "set1 = set(experiments[dataset].text1.tolist() + experiments[dataset].text2.tolist())\n",
    "set2 = set(sentences[dataset].index.tolist())\n",
    "\n",
    "for word in set1 - set2:\n",
    "    print('WORD: ' + word)\n",
    "    record = {'text': wn.synsets(word)[0].definition()}\n",
    "    for criteria in parsers:\n",
    "        if criteria == 'dep':\n",
    "            for start in ['left', 'right']:\n",
    "                record[criteria + '_' + start] = sort_text(record['text'], criteria, parsers, start)\n",
    "        else:\n",
    "            start = 'right'\n",
    "            record[criteria] = sort_text(record['text'], criteria, parsers, start)\n",
    "    \n",
    "    record['chunk'] = chunker(record['text'], lang_model, False)\n",
    "    sentences[dataset] = sentences[dataset].append(pd.Series(record, name=word))\n",
    "    sentences[dataset].sort_index(inplace=True)\n",
    "    \n",
    "print('Saving results to csv and pickle files...')\n",
    "sentences[dataset].to_csv(os.path.join(data_path, dataset, sentences_file[:-4] + '.csv'), sep='\\t')\n",
    "sentences[dataset].to_pickle(os.path.join(data_path, dataset, sentences_file))\n",
    "print('...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Word-Definition Dataset creation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first extract the words we want to appear in the dataset (Selected Option 2)\n",
    "# Option 1: Take the wordsim and men datasets\n",
    "# Option 2: Take a big corpus, lower the tokens, remove stopwords and select those that have a wordnet definition and\n",
    "#           a Google vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56057"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_toks = set(list(brown.words()))\n",
    "len(brown_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47522\n"
     ]
    }
   ],
   "source": [
    "brown_words = set([w.lower() for w in brown_toks if not re.findall('\\d', w)])\n",
    "print(len(brown_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36072\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the words to avoid having many repetitions of the same word skewing our results\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmas = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(w, pos=wn.NOUN), \n",
    "                                                                         pos=wn.VERB), pos=wn.ADV), pos=wn.ADJ) \n",
    "          for w in brown_words]\n",
    "lemmas = set(lemmas)\n",
    "print(len(lemmas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the stopwords file and the word2vec file\n",
    "WV_FILE = 'GoogleNews-vectors-negative300.bin'\n",
    "DATA_PATH2 = './data'\n",
    "\n",
    "with open(os.path.join(DATA_PATH, 'stopwords_en.txt'), 'r', encoding='utf-8') as f:\n",
    "    stopwords = [l.rstrip() for l in f.readlines()]\n",
    "\n",
    "word2vecs = KeyedVectors.load_word2vec_format(os.path.join(DATA_PATH2, WV_FILE), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20184"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only those lemmas that are not stopwords and do appear in both the wordnet defs and Google vectors\n",
    "filt_lemmas = [w for w in lemmas if w in word2vecs.vocab.keys() and w not in stopwords and wn.synsets(w)]\n",
    "len(filt_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happenstance',\n",
       " 'dialectically',\n",
       " 'incredible',\n",
       " 'greek',\n",
       " 'credential',\n",
       " 'traditionally',\n",
       " 'intercollegiate',\n",
       " 'exhibitor',\n",
       " 'yarrow',\n",
       " 'hesitate']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt_lemmas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary with the words as indexes and the first definition in wordnet as the unique column\n",
    "word_def = {w: wn.synsets(w)[0].definition() for w in filt_lemmas}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys, values = zip(*word_def.items())\n",
    "data = pd.DataFrame(values, index=keys, columns=['text'])\n",
    "data.drop(\"rock'n'roll\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>happenstance</th>\n",
       "      <td>an event that might have been arranged althoug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dialectically</th>\n",
       "      <td>in a dialectic manner</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incredible</th>\n",
       "      <td>beyond belief or understanding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greek</th>\n",
       "      <td>the Hellenic branch of the Indo-European famil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credential</th>\n",
       "      <td>a document attesting to the truth of certain s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            text\n",
       "happenstance   an event that might have been arranged althoug...\n",
       "dialectically                              in a dialectic manner\n",
       "incredible                        beyond belief or understanding\n",
       "greek          the Hellenic branch of the Indo-European famil...\n",
       "credential     a document attesting to the truth of certain s..."
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorting texts according to seq...\n",
      "Sorting texts according to syn...\n",
      "Sorting texts according to dep, starting at the left...\n",
      "Adding missed tokens: ['etc'] /// a precautionary measure warding off impending danger or damage or injury etc.\n",
      "Adding missed tokens: ['etc'] /// the official symbols of a family, state, etc.\n",
      "Adding missed tokens: ['etc'] /// any of several chemical elements that are usually shiny solids that conduct heat or electricity and can be formed into sheets etc.\n",
      "Adding missed tokens: ['etc'] /// any physical damage to the body caused by violence or accident or fracture etc.\n",
      "Adding missed tokens: ['etc'] /// a framework of steel bars raised on side supports to bridge over or around something; can display railway signals above several tracks or can support a traveling crane etc.\n",
      "Adding missed tokens: ['etc'] /// a measuring instrument for measuring and indicating a quantity such as the thickness of wire or the amount of rain etc.\n",
      "Adding missed tokens: ['etc'] /// any device that receives a signal or stimulus (as heat or pressure or light or motion etc.)\n",
      "Adding missed tokens: ['etc'] /// a person who gives private instruction (as in singing, acting, etc.)\n",
      "Adding missed tokens: ['etc'] /// an unexpected and inexplicable change in something (in a situation or a person's behavior, etc.)\n",
      "Adding missed tokens: ['etc'] /// relating to particularism (exclusive interest in one group or class or sect etc.)\n",
      "Adding missed tokens: ['etc'] /// a person who backs a politician or a team etc.\n",
      "Adding missed tokens: ['etc'] /// any device that receives a signal or stimulus (as heat or pressure or light or motion etc.)\n",
      "Adding missed tokens: ['etc'] /// a variety show with songs and comic acts etc.\n",
      "Adding missed tokens: ['etc'] /// a form of government in which the ruler is an absolute dictator (not restricted by a constitution or laws or opposition etc.)\n",
      "Adding missed tokens: ['etc'] /// be present at (meetings, church services, university), etc.\n",
      "Adding missed tokens: ['etc'] /// showing regard for others in manners, speech, behavior, etc.\n",
      "Adding missed tokens: ['etc'] /// (medicine) the act of caring for someone (as by medication or remedial training etc.)\n",
      "Adding missed tokens: ['etc'] /// a form of government in which the ruler is an absolute dictator (not restricted by a constitution or laws or opposition etc.)\n",
      "Adding missed tokens: ['etc'] /// in a subsequent part of this document or statement or matter etc.\n",
      "Adding missed tokens: ['etc'] /// a volatile flammable mixture of hydrocarbons (hexane and heptane and octane etc.)\n",
      "Adding missed tokens: ['etc'] /// any of various controls or devices for regulating or controlling fluid flow, pressure, temperature, etc.\n",
      "Adding missed tokens: ['etc'] /// a traveling show; having sideshows and rides and games of skill etc.\n",
      "Adding missed tokens: ['etc'] /// improvement (or an intended improvement) in the existing form or condition of institutions or practices etc.\n",
      "Adding missed tokens: ['etc'] /// the men and women who man a vehicle (ship, aircraft, etc.)\n",
      "Adding missed tokens: ['etc'] /// outer bark of the cork oak; used for stoppers for bottles etc.\n",
      "Adding missed tokens: ['etc'] /// the use of speech for informal exchange of views or ideas or information etc.\n",
      "Adding missed tokens: ['etc'] /// a gymnast who performs rolls and somersaults and twists etc.\n",
      "Adding missed tokens: ['etc'] /// a person who backs a politician or a team etc.\n",
      "Adding missed tokens: ['etc'] /// dry coloring material (especially a powder to be mixed with a liquid to produce paint, etc.)\n",
      "Adding missed tokens: ['etc'] /// accumulated wealth in the form of money or jewels etc.\n",
      "Adding missed tokens: ['etc'] /// compensation paid (to someone) for damages or losses or money already spent etc.\n",
      "Adding missed tokens: ['etc'] /// the act of losing or surrendering something as a penalty for a mistake or fault or failure to perform etc.\n",
      "Adding missed tokens: ['etc'] /// a way of regarding situations or topics etc.\n",
      "Adding missed tokens: ['etc'] /// the tableware (plates and platters and serving bowls etc.)\n",
      "Adding missed tokens: ['etc'] /// the act of intervening (as to mediate a dispute, etc.)\n",
      "Adding missed tokens: ['etc'] /// a worker who bleaches (cloth or flour etc.)\n",
      "Adding missed tokens: ['etc'] /// the act of getting recruits; enlisting people for the army (or for a job or a cause etc.)\n",
      "Adding missed tokens: ['etc'] /// the act of being present (at a meeting or event etc.)\n",
      "Adding missed tokens: ['etc'] /// make more attractive by adding ornament, colour, etc.\n",
      "Adding missed tokens: ['etc'] /// make more attractive by adding ornament, colour, etc.\n",
      "Adding missed tokens: ['etc'] /// the act of vindicating or defending against criticism or censure etc.\n",
      "Adding missed tokens: ['etc'] /// the brush (small trees and bushes and ferns etc.)\n",
      "Adding missed tokens: ['etc'] /// a list of items (names or tasks etc.)\n",
      "Adding missed tokens: ['etc'] /// a hypothetical possibility, circumstance, statement, proposal, situation, etc.\n",
      "Adding missed tokens: ['etc'] /// an ostentatious display (of effort or extravagance etc.)\n",
      "Adding missed tokens: ['etc'] /// the brush (small trees and bushes and ferns etc.)\n",
      "Adding missed tokens: ['etc'] /// a defense of some offensive behavior or some failure to keep a promise etc.\n",
      "Adding missed tokens: ['etc'] /// a form of government in which the ruler is an absolute dictator (not restricted by a constitution or laws or opposition etc.)\n",
      "Adding missed tokens: ['etc'] /// a list of writings with time and place of publication (such as the writings of a single author or the works referred to in preparing a document etc.)\n",
      "Adding missed tokens: ['etc'] /// mustards: cabbages; cauliflowers; turnips; etc.\n",
      "Adding missed tokens: ['etc'] /// make laws, bills, etc.\n",
      "Adding missed tokens: ['etc'] /// equipment consisting of miscellaneous articles needed for a particular operation or sport etc.\n",
      "Adding missed tokens: ['etc'] /// someone who leads or initiates an activity (attack or campaign etc.)\n",
      "Adding missed tokens: ['etc'] /// a person who backs a politician or a team etc.\n",
      "Adding missed tokens: ['etc'] /// a tool for tamping (e.g., for tamping tobacco into a pipe bowl or a charge into a drill hole etc.)\n",
      "Adding missed tokens: ['etc'] /// an accidental hole that allows something (fluid or light etc.)\n",
      "Adding missed tokens: ['etc'] /// a flat (usually rectangular) container for a letter, thin package, etc.\n",
      "Adding missed tokens: ['etc'] /// the act of sowing (of seeds in the ground or, figuratively, of germs in the body or ideas in the mind, etc.)\n",
      "Adding missed tokens: ['etc'] /// an attack by a defending force against an attacking enemy force in order to regain lost ground or cut off enemy advance units etc.\n",
      "Adding missed tokens: ['etc'] /// a person who backs a politician or a team etc.\n",
      "Adding missed tokens: ['etc'] /// having power or capacity or tendency to absorb or soak up something (liquids or energy etc.)\n",
      "Adding missed tokens: ['etc'] /// a statistic characterizing human populations (or segments of human populations broken down by age or sex or income etc.)\n",
      "Adding missed tokens: ['etc'] /// in such a manner that death ensues (also in reference to hatred, jealousy, fear, etc.)\n",
      "Adding missed tokens: ['etc'] /// protective covering consisting of a panel to protect people from the splashing water or mud etc.\n",
      "Adding missed tokens: ['etc'] /// someone who presents a message of some sort (as a petition or an address or a check or a memorial etc.)\n",
      "Adding missed tokens: ['etc'] /// the chair of state for a monarch, bishop, etc.\n",
      "Adding missed tokens: ['etc'] /// any physical damage to the body caused by violence or accident or fracture etc.\n",
      "Adding missed tokens: ['etc'] /// a transparent paperlike product that is impervious to moisture and used to wrap candy or cigarettes etc.\n",
      "Adding missed tokens: ['etc'] /// an emblem (a small piece of plastic or cloth or metal) that signifies your status (rank or membership or affiliation etc.)\n",
      "Adding missed tokens: ['etc'] /// something (manuscripts or architectural plans and models or estimates or works of art of all genres etc.)\n",
      "Adding missed tokens: ['etc'] /// a statement that makes something comprehensible by describing the relevant structure or operation or circumstances etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding missed tokens: ['etc'] /// one of a series of rounded projections (or the notches between them) formed by curves along an edge (as the edge of a leaf or piece of cloth or the margin of a shell or a shriveled red blood cell observed in a hypertonic solution etc.)\n",
      "Adding missed tokens: ['etc'] /// a precautionary measure warding off impending danger or damage or injury etc.\n",
      "Adding missed tokens: ['etc'] /// a medical dressing consisting of a soft heated mass of meal or clay that is spread on a cloth and applied to the skin to treat inflamed areas or improve circulation etc.\n",
      "Adding missed tokens: ['etc'] /// a label written or printed on paper, cardboard, or plastic that is attached to something to indicate its owner, nature, price, etc.\n",
      "Adding missed tokens: ['etc'] /// a way of regarding situations or topics etc.\n",
      "Adding missed tokens: ['etc'] /// active support of an idea or cause etc.\n",
      "Adding missed tokens: ['etc'] /// a disloyal person who betrays or deserts his cause or religion or political party or friend etc.\n",
      "Adding missed tokens: ['etc'] /// creative activity (writing or pictures or films etc.)\n",
      "Adding missed tokens: ['etc'] /// a performance of a musical composition or a dramatic role etc.\n",
      "Adding missed tokens: ['etc'] /// a suspension of insoluble particles (as plaster of Paris or lime or clay etc.)\n",
      "Adding missed tokens: ['etc'] /// the act of dismounting (a horse or bike etc.)\n",
      "Adding missed tokens: ['etc'] /// a map showing planned or actual features of an area (streets and building lots etc.)\n",
      "Adding missed tokens: ['etc'] /// an artist who makes illustrations (for books or magazines or advertisements etc.)\n",
      "Adding missed tokens: ['etc'] /// any physical damage to the body caused by violence or accident or fracture etc.\n",
      "Adding missed tokens: ['etc'] /// a fine cord of twisted fibers (of cotton or silk or wool or nylon etc.)\n",
      "Adding missed tokens: ['etc'] /// something that is emitted or radiated (as a gas or an odor or a light, etc.)\n",
      "Adding missed tokens: ['etc'] /// a person who is opposed (to an action or policy or practice etc.)\n",
      "Adding missed tokens: ['etc'] /// an officer of the court who is employed to execute writs and processes and make arrests etc.\n",
      "Adding missed tokens: ['etc'] /// a workplace where dairy products (butter and cheese etc.)\n",
      "Adding missed tokens: ['etc'] /// (usually plural) a device with an explosive that burns at a low rate and with colored flames; can be used to illuminate areas or send signals etc.\n",
      "Adding missed tokens: ['etc'] /// the utterance of a sound similar to clearing the throat; intended to get attention, express hesitancy, fill a pause, hide embarrassment, warn a friend, etc.\n",
      "Adding missed tokens: ['etc'] /// the brush (small trees and bushes and ferns etc.)\n",
      "Adding missed tokens: ['etc'] /// any physical damage to the body caused by violence or accident or fracture etc.\n",
      "Adding missed tokens: ['etc'] /// a form of government in which the ruler is an absolute dictator (not restricted by a constitution or laws or opposition etc.)\n",
      "Adding missed tokens: ['etc'] /// a high standing achieved through success or influence or wealth etc.\n",
      "Adding missed tokens: ['etc'] /// write out from speech, notes, etc.\n",
      "Adding missed tokens: ['etc'] /// a tool for tamping (e.g., for tamping tobacco into a pipe bowl or a charge into a drill hole etc.)\n",
      "Adding missed tokens: ['etc'] /// an enclosed chamber in which heat is produced to heat buildings, destroy refuse, smelt or refine ores, etc.\n",
      "Adding missed tokens: ['etc'] /// a barrier set up by police to stop traffic on a street or road in order to catch a fugitive or inspect traffic etc.\n",
      "Adding missed tokens: ['etc'] /// the time between two reigns, governments, etc.\n",
      "Adding missed tokens: ['etc'] /// direct or control; projects, businesses, etc.\n",
      "Adding missed tokens: ['etc'] /// one side of one leaf (of a book or magazine or newspaper or letter etc.)\n",
      "Adding missed tokens: ['etc'] /// issue or terminate (in a specified way, state, etc.\n",
      "Adding missed tokens: ['etc'] /// any of various minerals consisting of hydrous silicates of aluminum or potassium etc.\n",
      "Adding missed tokens: ['etc'] /// a structure that allows people or vehicles to cross an obstacle such as a river or canal or railway etc.\n",
      "Sorting texts according to dep, starting at the right...\n",
      "Adding missed tokens: ['etc'] /// a precautionary measure warding off impending danger or damage or injury etc.\n",
      "Adding missed tokens: ['etc'] /// the official symbols of a family, state, etc.\n",
      "Adding missed tokens: ['etc'] /// any of several chemical elements that are usually shiny solids that conduct heat or electricity and can be formed into sheets etc.\n",
      "Adding missed tokens: ['etc'] /// any physical damage to the body caused by violence or accident or fracture etc.\n",
      "Adding missed tokens: ['etc'] /// a framework of steel bars raised on side supports to bridge over or around something; can display railway signals above several tracks or can support a traveling crane etc.\n",
      "Adding missed tokens: ['etc'] /// a measuring instrument for measuring and indicating a quantity such as the thickness of wire or the amount of rain etc.\n",
      "Adding missed tokens: ['etc'] /// any device that receives a signal or stimulus (as heat or pressure or light or motion etc.)\n",
      "Adding missed tokens: ['etc'] /// a person who gives private instruction (as in singing, acting, etc.)\n",
      "Adding missed tokens: ['etc'] /// an unexpected and inexplicable change in something (in a situation or a person's behavior, etc.)\n",
      "Adding missed tokens: ['etc'] /// relating to particularism (exclusive interest in one group or class or sect etc.)\n",
      "Adding missed tokens: ['etc'] /// a person who backs a politician or a team etc.\n",
      "Adding missed tokens: ['etc'] /// any device that receives a signal or stimulus (as heat or pressure or light or motion etc.)\n",
      "Adding missed tokens: ['etc'] /// a variety show with songs and comic acts etc.\n",
      "Adding missed tokens: ['etc'] /// a form of government in which the ruler is an absolute dictator (not restricted by a constitution or laws or opposition etc.)\n",
      "Adding missed tokens: ['etc'] /// be present at (meetings, church services, university), etc.\n",
      "Adding missed tokens: ['etc'] /// showing regard for others in manners, speech, behavior, etc.\n",
      "Adding missed tokens: ['etc'] /// (medicine) the act of caring for someone (as by medication or remedial training etc.)\n",
      "Adding missed tokens: ['etc'] /// a form of government in which the ruler is an absolute dictator (not restricted by a constitution or laws or opposition etc.)\n",
      "Adding missed tokens: ['etc'] /// in a subsequent part of this document or statement or matter etc.\n",
      "Adding missed tokens: ['etc'] /// a volatile flammable mixture of hydrocarbons (hexane and heptane and octane etc.)\n",
      "Adding missed tokens: ['etc'] /// any of various controls or devices for regulating or controlling fluid flow, pressure, temperature, etc.\n",
      "Adding missed tokens: ['etc'] /// a traveling show; having sideshows and rides and games of skill etc.\n",
      "Adding missed tokens: ['etc'] /// improvement (or an intended improvement) in the existing form or condition of institutions or practices etc.\n",
      "Adding missed tokens: ['etc'] /// the men and women who man a vehicle (ship, aircraft, etc.)\n",
      "Adding missed tokens: ['etc'] /// outer bark of the cork oak; used for stoppers for bottles etc.\n",
      "Adding missed tokens: ['etc'] /// the use of speech for informal exchange of views or ideas or information etc.\n",
      "Adding missed tokens: ['etc'] /// a gymnast who performs rolls and somersaults and twists etc.\n",
      "Adding missed tokens: ['etc'] /// a person who backs a politician or a team etc.\n",
      "Adding missed tokens: ['etc'] /// dry coloring material (especially a powder to be mixed with a liquid to produce paint, etc.)\n",
      "Adding missed tokens: ['etc'] /// accumulated wealth in the form of money or jewels etc.\n",
      "Adding missed tokens: ['etc'] /// compensation paid (to someone) for damages or losses or money already spent etc.\n",
      "Adding missed tokens: ['etc'] /// the act of losing or surrendering something as a penalty for a mistake or fault or failure to perform etc.\n",
      "Adding missed tokens: ['etc'] /// a way of regarding situations or topics etc.\n",
      "Adding missed tokens: ['etc'] /// the tableware (plates and platters and serving bowls etc.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding missed tokens: ['etc'] /// the act of intervening (as to mediate a dispute, etc.)\n",
      "Adding missed tokens: ['etc'] /// a worker who bleaches (cloth or flour etc.)\n",
      "Adding missed tokens: ['etc'] /// the act of getting recruits; enlisting people for the army (or for a job or a cause etc.)\n",
      "Adding missed tokens: ['etc'] /// the act of being present (at a meeting or event etc.)\n",
      "Adding missed tokens: ['etc'] /// make more attractive by adding ornament, colour, etc.\n",
      "Adding missed tokens: ['etc'] /// make more attractive by adding ornament, colour, etc.\n",
      "Adding missed tokens: ['etc'] /// the act of vindicating or defending against criticism or censure etc.\n",
      "Adding missed tokens: ['etc'] /// the brush (small trees and bushes and ferns etc.)\n",
      "Adding missed tokens: ['etc'] /// a list of items (names or tasks etc.)\n",
      "Adding missed tokens: ['etc'] /// a hypothetical possibility, circumstance, statement, proposal, situation, etc.\n",
      "Adding missed tokens: ['etc'] /// an ostentatious display (of effort or extravagance etc.)\n",
      "Adding missed tokens: ['etc'] /// the brush (small trees and bushes and ferns etc.)\n",
      "Adding missed tokens: ['etc'] /// a defense of some offensive behavior or some failure to keep a promise etc.\n",
      "Adding missed tokens: ['etc'] /// a form of government in which the ruler is an absolute dictator (not restricted by a constitution or laws or opposition etc.)\n",
      "Adding missed tokens: ['etc'] /// a list of writings with time and place of publication (such as the writings of a single author or the works referred to in preparing a document etc.)\n",
      "Adding missed tokens: ['etc'] /// mustards: cabbages; cauliflowers; turnips; etc.\n",
      "Adding missed tokens: ['etc'] /// make laws, bills, etc.\n",
      "Adding missed tokens: ['etc'] /// equipment consisting of miscellaneous articles needed for a particular operation or sport etc.\n",
      "Adding missed tokens: ['etc'] /// someone who leads or initiates an activity (attack or campaign etc.)\n",
      "Adding missed tokens: ['etc'] /// a person who backs a politician or a team etc.\n",
      "Adding missed tokens: ['etc'] /// a tool for tamping (e.g., for tamping tobacco into a pipe bowl or a charge into a drill hole etc.)\n",
      "Adding missed tokens: ['etc'] /// an accidental hole that allows something (fluid or light etc.)\n",
      "Adding missed tokens: ['etc'] /// a flat (usually rectangular) container for a letter, thin package, etc.\n",
      "Adding missed tokens: ['etc'] /// the act of sowing (of seeds in the ground or, figuratively, of germs in the body or ideas in the mind, etc.)\n",
      "Adding missed tokens: ['etc'] /// an attack by a defending force against an attacking enemy force in order to regain lost ground or cut off enemy advance units etc.\n",
      "Adding missed tokens: ['etc'] /// a person who backs a politician or a team etc.\n",
      "Adding missed tokens: ['etc'] /// having power or capacity or tendency to absorb or soak up something (liquids or energy etc.)\n",
      "Adding missed tokens: ['etc'] /// a statistic characterizing human populations (or segments of human populations broken down by age or sex or income etc.)\n",
      "Adding missed tokens: ['etc'] /// in such a manner that death ensues (also in reference to hatred, jealousy, fear, etc.)\n",
      "Adding missed tokens: ['etc'] /// protective covering consisting of a panel to protect people from the splashing water or mud etc.\n",
      "Adding missed tokens: ['etc'] /// someone who presents a message of some sort (as a petition or an address or a check or a memorial etc.)\n",
      "Adding missed tokens: ['etc'] /// the chair of state for a monarch, bishop, etc.\n",
      "Adding missed tokens: ['etc'] /// any physical damage to the body caused by violence or accident or fracture etc.\n",
      "Adding missed tokens: ['etc'] /// a transparent paperlike product that is impervious to moisture and used to wrap candy or cigarettes etc.\n",
      "Adding missed tokens: ['etc'] /// an emblem (a small piece of plastic or cloth or metal) that signifies your status (rank or membership or affiliation etc.)\n",
      "Adding missed tokens: ['etc'] /// something (manuscripts or architectural plans and models or estimates or works of art of all genres etc.)\n",
      "Adding missed tokens: ['etc'] /// a statement that makes something comprehensible by describing the relevant structure or operation or circumstances etc.\n",
      "Adding missed tokens: ['etc'] /// one of a series of rounded projections (or the notches between them) formed by curves along an edge (as the edge of a leaf or piece of cloth or the margin of a shell or a shriveled red blood cell observed in a hypertonic solution etc.)\n",
      "Adding missed tokens: ['etc'] /// a precautionary measure warding off impending danger or damage or injury etc.\n",
      "Adding missed tokens: ['etc'] /// a medical dressing consisting of a soft heated mass of meal or clay that is spread on a cloth and applied to the skin to treat inflamed areas or improve circulation etc.\n",
      "Adding missed tokens: ['etc'] /// a label written or printed on paper, cardboard, or plastic that is attached to something to indicate its owner, nature, price, etc.\n",
      "Adding missed tokens: ['etc'] /// a way of regarding situations or topics etc.\n",
      "Adding missed tokens: ['etc'] /// active support of an idea or cause etc.\n",
      "Adding missed tokens: ['etc'] /// a disloyal person who betrays or deserts his cause or religion or political party or friend etc.\n",
      "Adding missed tokens: ['etc'] /// creative activity (writing or pictures or films etc.)\n",
      "Adding missed tokens: ['etc'] /// a performance of a musical composition or a dramatic role etc.\n",
      "Adding missed tokens: ['etc'] /// a suspension of insoluble particles (as plaster of Paris or lime or clay etc.)\n",
      "Adding missed tokens: ['etc'] /// the act of dismounting (a horse or bike etc.)\n",
      "Adding missed tokens: ['etc'] /// a map showing planned or actual features of an area (streets and building lots etc.)\n",
      "Adding missed tokens: ['etc'] /// an artist who makes illustrations (for books or magazines or advertisements etc.)\n",
      "Adding missed tokens: ['etc'] /// any physical damage to the body caused by violence or accident or fracture etc.\n",
      "Adding missed tokens: ['etc'] /// a fine cord of twisted fibers (of cotton or silk or wool or nylon etc.)\n",
      "Adding missed tokens: ['etc'] /// something that is emitted or radiated (as a gas or an odor or a light, etc.)\n",
      "Adding missed tokens: ['etc'] /// a person who is opposed (to an action or policy or practice etc.)\n",
      "Adding missed tokens: ['etc'] /// an officer of the court who is employed to execute writs and processes and make arrests etc.\n",
      "Adding missed tokens: ['etc'] /// a workplace where dairy products (butter and cheese etc.)\n",
      "Adding missed tokens: ['etc'] /// (usually plural) a device with an explosive that burns at a low rate and with colored flames; can be used to illuminate areas or send signals etc.\n",
      "Adding missed tokens: ['etc'] /// the utterance of a sound similar to clearing the throat; intended to get attention, express hesitancy, fill a pause, hide embarrassment, warn a friend, etc.\n",
      "Adding missed tokens: ['etc'] /// the brush (small trees and bushes and ferns etc.)\n",
      "Adding missed tokens: ['etc'] /// any physical damage to the body caused by violence or accident or fracture etc.\n",
      "Adding missed tokens: ['etc'] /// a form of government in which the ruler is an absolute dictator (not restricted by a constitution or laws or opposition etc.)\n",
      "Adding missed tokens: ['etc'] /// a high standing achieved through success or influence or wealth etc.\n",
      "Adding missed tokens: ['etc'] /// write out from speech, notes, etc.\n",
      "Adding missed tokens: ['etc'] /// a tool for tamping (e.g., for tamping tobacco into a pipe bowl or a charge into a drill hole etc.)\n",
      "Adding missed tokens: ['etc'] /// an enclosed chamber in which heat is produced to heat buildings, destroy refuse, smelt or refine ores, etc.\n",
      "Adding missed tokens: ['etc'] /// a barrier set up by police to stop traffic on a street or road in order to catch a fugitive or inspect traffic etc.\n",
      "Adding missed tokens: ['etc'] /// the time between two reigns, governments, etc.\n",
      "Adding missed tokens: ['etc'] /// direct or control; projects, businesses, etc.\n",
      "Adding missed tokens: ['etc'] /// one side of one leaf (of a book or magazine or newspaper or letter etc.)\n",
      "Adding missed tokens: ['etc'] /// issue or terminate (in a specified way, state, etc.\n",
      "Adding missed tokens: ['etc'] /// any of various minerals consisting of hydrous silicates of aluminum or potassium etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding missed tokens: ['etc'] /// a structure that allows people or vehicles to cross an obstacle such as a river or canal or railway etc.\n",
      "Saving results to csv and pickle files...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fname = 'word_defs'\n",
    "data_path = './DATA_SETS'\n",
    "path = 'WORD_DEF'\n",
    "\n",
    "# Sort the text according to nltk\n",
    "for criteria in parsers:\n",
    "    if criteria == 'dep':\n",
    "        for start in ['left', 'right']:\n",
    "            print('Sorting texts according to ' + criteria + ', starting at the ' + start + '...')\n",
    "            data[criteria + '_' + start] = data[['text']].apply(lambda x: \n",
    "                                                                sort_text(x['text'], criteria, parsers, start), \n",
    "                                                                axis=1)\n",
    "    else:\n",
    "        print('Sorting texts according to ' + criteria + '...')\n",
    "        start = 'right'\n",
    "        data[criteria] = data[['text']].apply(lambda x: sort_text(x['text'], criteria, parsers, start), \n",
    "                                              axis=1)\n",
    "\n",
    "print('Saving results to csv and pickle files...')\n",
    "data.to_csv(os.path.join(data_path, path, fname + '.csv'), sep='\\t')\n",
    "data.to_pickle(os.path.join(data_path, path, fname + '.pkl'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file word_defs...\n",
      "Saving results to csv and pickle files...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lang_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "fname = 'word_defs'\n",
    "data_path = './DATA_SETS'\n",
    "path = 'WORD_DEF'\n",
    "print('Processing file ' + fname + '...')\n",
    "\n",
    "data = pd.read_pickle(os.path.join(data_path, path, fname + '.pkl'))\n",
    "data.drop(\"rock'n'roll\", inplace=True)\n",
    "data['chunk'] = data[['text']].apply(lambda x: chunker(x['text'], lang_model, False), axis=1)\n",
    "\n",
    "print('Saving results to csv and pickle files...')\n",
    "data.to_csv(os.path.join(data_path, path, fname + '.csv'), sep='\\t')\n",
    "data.to_pickle(os.path.join(data_path, path, fname + '.pkl'))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'word_defs'\n",
    "data_path = './DATA_SETS'\n",
    "path = 'WORD_DEF'\n",
    "\n",
    "data = pd.read_pickle(os.path.join(data_path, path, fname + '.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>seq</th>\n",
       "      <th>syn</th>\n",
       "      <th>dep_left</th>\n",
       "      <th>dep_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>happenstance</th>\n",
       "      <td>an event that might have been arranged althoug...</td>\n",
       "      <td>[an, event, that, might, have, been, arranged,...</td>\n",
       "      <td>[[an, event], [that, [might, ['have', ['been',...</td>\n",
       "      <td>[event, an, [arranged, that, might, have, been...</td>\n",
       "      <td>[an, [that, might, have, been, [although, it, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dialectically</th>\n",
       "      <td>in a dialectic manner</td>\n",
       "      <td>[in, a, dialectic, manner]</td>\n",
       "      <td>[in, [a, dialectic, manner]]</td>\n",
       "      <td>[manner, in, a, dialectic]</td>\n",
       "      <td>[in, a, dialectic, manner]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incredible</th>\n",
       "      <td>beyond belief or understanding</td>\n",
       "      <td>[beyond, belief, or, understanding]</td>\n",
       "      <td>[beyond, [belief, or, understanding]]</td>\n",
       "      <td>[belief, beyond, or, understanding]</td>\n",
       "      <td>[beyond, or, understanding, belief]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>greek</th>\n",
       "      <td>the Hellenic branch of the Indo-European famil...</td>\n",
       "      <td>[the, Hellenic, branch, of, the, Indo-European...</td>\n",
       "      <td>[[the, Hellenic, branch], [of, [['the', 'Indo-...</td>\n",
       "      <td>[branch, the, Hellenic, [family, of, the, Indo...</td>\n",
       "      <td>[the, Hellenic, [of, the, Indo-European, [of, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>credential</th>\n",
       "      <td>a document attesting to the truth of certain s...</td>\n",
       "      <td>[a, document, attesting, to, the, truth, of, c...</td>\n",
       "      <td>[[a, document], [attesting, [to, [['the', 'tru...</td>\n",
       "      <td>[attesting, a, document, [truth, to, the, [fac...</td>\n",
       "      <td>[a, document, [to, the, [of, certain, stated, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            text  \\\n",
       "happenstance   an event that might have been arranged althoug...   \n",
       "dialectically                              in a dialectic manner   \n",
       "incredible                        beyond belief or understanding   \n",
       "greek          the Hellenic branch of the Indo-European famil...   \n",
       "credential     a document attesting to the truth of certain s...   \n",
       "\n",
       "                                                             seq  \\\n",
       "happenstance   [an, event, that, might, have, been, arranged,...   \n",
       "dialectically                         [in, a, dialectic, manner]   \n",
       "incredible                   [beyond, belief, or, understanding]   \n",
       "greek          [the, Hellenic, branch, of, the, Indo-European...   \n",
       "credential     [a, document, attesting, to, the, truth, of, c...   \n",
       "\n",
       "                                                             syn  \\\n",
       "happenstance   [[an, event], [that, [might, ['have', ['been',...   \n",
       "dialectically                       [in, [a, dialectic, manner]]   \n",
       "incredible                 [beyond, [belief, or, understanding]]   \n",
       "greek          [[the, Hellenic, branch], [of, [['the', 'Indo-...   \n",
       "credential     [[a, document], [attesting, [to, [['the', 'tru...   \n",
       "\n",
       "                                                        dep_left  \\\n",
       "happenstance   [event, an, [arranged, that, might, have, been...   \n",
       "dialectically                         [manner, in, a, dialectic]   \n",
       "incredible                   [belief, beyond, or, understanding]   \n",
       "greek          [branch, the, Hellenic, [family, of, the, Indo...   \n",
       "credential     [attesting, a, document, [truth, to, the, [fac...   \n",
       "\n",
       "                                                       dep_right  \n",
       "happenstance   [an, [that, might, have, been, [although, it, ...  \n",
       "dialectically                         [in, a, dialectic, manner]  \n",
       "incredible                   [beyond, or, understanding, belief]  \n",
       "greek          [the, Hellenic, [of, the, Indo-European, [of, ...  \n",
       "credential     [a, document, [to, the, [of, certain, stated, ...  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       ROOT                  \n",
      "                        |                     \n",
      "                        S                    \n",
      "       _________________|____                 \n",
      "      |                      VP              \n",
      "      |            __________|___             \n",
      "      |           |     |        PP          \n",
      "      |           |     |     ___|___         \n",
      "      NP          |    PRT   |       NP      \n",
      "  ____|____       |     |    |    ___|____    \n",
      " RB       NNP    VBD    RP   IN  DT       NN \n",
      " |         |      |     |    |   |        |   \n",
      "Just     Robert showed  up  for the     party\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.44.1 (20200629.0846)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"290pt\" height=\"305pt\"\n",
       " viewBox=\"0.00 0.00 289.50 305.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 301)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-301 285.5,-301 285.5,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">3 (showed)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;3 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118.5,-260.8C118.5,-249.16 118.5,-233.55 118.5,-220.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122,-220.18 118.5,-210.18 115,-220.18 122,-220.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"137\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">ROOT</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"36.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">2 (Robert)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>3&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M101.88,-173.93C96.34,-168.23 90.15,-161.85 84.5,-156 76.6,-147.82 67.99,-138.87 60.28,-130.82\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.36,-127.94 52.91,-123.15 57.31,-132.79 62.36,-127.94\"/>\n",
       "<text text-anchor=\"middle\" x=\"99.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nsubj</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"118.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">4 (up)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M118.5,-173.8C118.5,-162.16 118.5,-146.55 118.5,-133.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"122,-133.18 118.5,-123.18 115,-133.18 122,-133.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"157\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">compound:prt</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"218.5\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">7 (party)</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;7 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M157.24,-180.06C171,-174.63 185.73,-166.85 196.5,-156 202.94,-149.52 207.64,-140.84 211.01,-132.53\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"214.31,-133.69 214.37,-123.1 207.72,-131.34 214.31,-133.69\"/>\n",
       "<text text-anchor=\"middle\" x=\"222.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">nmod</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"36.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">1 (Just)</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;1 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M36.5,-86.8C36.5,-75.16 36.5,-59.55 36.5,-46.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"40,-46.18 36.5,-36.18 33,-46.18 40,-46.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"59\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">advmod</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"182.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">5 (for)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M211.21,-86.8C206.19,-74.93 199.41,-58.93 193.7,-45.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"196.9,-44.02 189.77,-36.18 190.45,-46.75 196.9,-44.02\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">case</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"254.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">6 (the)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M225.79,-86.8C230.81,-74.93 237.59,-58.93 243.3,-45.45\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"246.55,-46.75 247.23,-36.18 240.1,-44.02 246.55,-46.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"247\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">det</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<DependencyGraph with 8 nodes>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = 'Just Robert showed up for the party'\n",
    "tree = next(parsers['syn'].raw_parse(sent))\n",
    "tree.pretty_print()\n",
    "next(parsers['dep'].raw_parse(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"SEQ-L2R: {sort_text(sent, 'seq', parsers, 'left')}\")\n",
    "print(f\"SYN-L2R: {sort_text(sent, 'syn', parsers, 'left')}\")\n",
    "print(f\"DEP-L2R: {sort_text(sent, 'dep', parsers, 'left')}\")\n",
    "print(f\"DEP-R2L: {sort_text(sent, 'dep', parsers, 'right')}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ-L2R: ['Just', 'Robert', 'showed', 'up', 'for', 'the', 'party']\n",
      "SYN-L2R: [['Just', 'Robert'], ['showed', 'up', ['for', ['the', 'party']]]]\n",
      "DEP-L2R: ['showed', ['Robert', 'Just'], 'up', ['party', 'for', 'the']]\n",
      "DEP-R2L: [['Just', 'Robert'], 'up', ['for', 'the', 'party'], 'showed']\n"
     ]
    }
   ],
   "source": [
    "---\n",
    "\n",
    "## Word-Definition Dataset creation 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Word-Definition Dataset creation 2\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data'\n",
    "WV_FILE = 'GoogleNews-vectors-negative300.bin'\n",
    "FILENAME = 'wordsim353.csv'\n",
    "\n",
    "parsers = {'seq': CoreNLPParser(url='http://localhost:9000'),\n",
    "           'syn': CoreNLPParser(url='http://localhost:9000'),\n",
    "           'dep': CoreNLPDependencyParser(url='http://localhost:9000')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vecs = KeyedVectors.load_word2vec_format(os.path.join(DATA_PATH, WV_FILE), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsims = pd.read_csv(os.path.join(DATA_PATH, FILENAME), sep=',')\n",
    "wordsim_words = set(wordsims[\"Word 1\"].tolist() + wordsims[\"Word 2\"].tolist())\n",
    "wordsim_words = sorted(list([w for w in wordsim_words if w in word2vecs.vocab.keys() and wn.synsets(w)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}